\section{Motivation}
Compressing a deep learning model into a smaller size typically leads to a smaller model footprint, but it does not always lead to faster
inference time and lower energy consumption. As a motivation example, considering applying \FIXME{four} commonly used model compression
techniques to \FIXME{xx} influential deep learning models -- including \FIXME{xx} \CNNs and \FIXME{RNNs}. Our evaluation platform is a
NVIDIA Jetson TX2 embedded deep learning platform with Tensorflow Mobile \FIXME{v.xx} (see Section~\ref{sec:platform}).

Figure~\ref{} summarizes the change of the storage size, memory footprint, inference time, and energy consumption after applying each model
compression technique.
