\section{Introduction}
In recent years, deep learning has emerged as a powerful tool for solving complex problems that were considered to be difficult in the
past. It has brought a step change in the machine's ability to solve tasks like object recognition~\cite{donahue14,he2016deep}, facial
recognition~\cite{parkhi2015deep,sun2014deep}, speech processing~\cite{pmlrv48amodei16}, and machine translation~\cite{bahdanau2014neural}.
While many of these tasks are also important on mobiles and the Internet of Things (IoT), existing solutions are often
computation-intensive and require a large amount of resources for the model to operate. As a result, performing deep
inference\footnote{Inference in this paper refers to apply a pre-trained model on an input to obtain the corresponding output. This is
different from statistical inference.} on embedded devices can lead to long runtimes and the consumption of abundant amounts of resources,
including CPU, memory, and power, even for simple tasks~\cite{CanzianiPC16}. Without a solution,
 the hoped-for advances on embedded sensing will not arrive.


Approaches have been proposed to accelerate deep inference on embedded devices. These include designing purpose-built hardware to reduce
the computation or memory latency~\cite{georgiev2017low}, compressing a pre-trained model to reduce its storage and memory footprint as
well as computational requirements~\cite{han2016eie}, and offloading some, or all, computation to a cloud
server~\cite{Kang2017neurosurgeon,teerapittayanon2017distributed}. Compared to specialized hardware, software-based model compression
techniques have the advantage of being readily deployable on commercial-off-the-self hardware; and compared to computation offloading,
model compression enables local, on-device inference which in turn allows faster response time and is less privacy intrusive. Such
advantages make model compressions attractive on resource-constrained IoT devices where computation offloading is infeasible.


However, model compression is not a free lunch as it comes at the cost of loss in prediction accuracy\FIXME{~\cite{cheng2017survey}}. This
means that one must carefully choose the model compression technique and its parameters to effectively trade precision for computation and
resource requirements or less energy consumption. Furthermore, as we will show in this paper, the reduction in the model size does not
necessarily translate into faster inference time. Because there is no guarantee for a compression technique to be beneficial, we need to
understand when and how to apply a model compression technique.

Our work aims to characterize deep learning model compression techniques for embedded inference. Knowing this not only informs the better
deployment of computation-intensive models, but also enables destining more efficient architectures for models and hardware for mobile and
IoT devices.

To that end, we develop a quantitative approach  to characterize two mainstream model compression techniques,
pruning\FIXME{~\cite{cheng2017survey}} and data quantization~\cite{Gong2014Compressing}. We apply the techniques to the image
classification and the natural language processing (NLP) domains, two areas where deep learning has made great breakthroughs and a rich set
of pre-trained models are available. We evaluate the compression results on the NVIDIA Jetson TX2 embedded deep learning platform and
consider a wide range of influential deep learning models. We use the 50K images from the ImageNet ILSVRC 2012 validation
dataset~\cite{imagenet2012} for image classification and a corpus of ten thousand text samples\FIXME{~\cite{}} for NLP.


We show that while there is significant gain for choosing the right compression technique and parameters, mistakes can seriously hurt the
performance. We then quantify how different model compression techniques and parameters affect the inference time, energy consumption,
model storage requirement and the prediction accuracy. As a result, our work provides insights on when and how to apply deep learning model
compression techniques on embedded devices.

The main contributions of this paper are two folds:

\begin{itemize}
\item We present an extensive study to characterize and understand how two popular model compression techniques perform on a
    representative embedded deep learning platform;
\item Our work offers insights on when and how to apply compression techniques for embedded deep inference.
\end{itemize}
