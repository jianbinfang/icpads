\section{Introduction}
In recent years, CNN has become the main technical means of computer vision tasks, and it has been splendid in the direction of image classification, target detection, depth estimation, and semantic segmentation. In particular, since AlexNet won the ILSVRC 2012 ImageNet Image Classification Competition in one fell swoop, the deep neural network boom has swept the entire field of computer vision. Depth model swiftness replaces traditional manual design features and classifiers. Various depth models not only provide an end-to-end processing method, but also significantly refresh the accuracy of each task, and even beyond the accuracy of human knowledge. So far, the accuracy of the image classification model has reached more than 95\% (top5).

However, behind the excellent performance of the depth model is a powerful computing demand: while constantly refreshing the limits of the accuracy of the task, its depth and size are also exponentially increasing. In order to preserve user privacy and reduce user-aware query time, these deep neural networks need to be migrated to various new platform applications, including mobile platforms, autonomous systems, and smart devices. Therefore, one of the following problems is that such a huge model can only be used on a limited platform, it cannot be ported to mobile terminals and embedded chips at all, and the high bandwidth consumption is also daunting to many users. On the other hand, large-scale models also pose enormous challenges to equipment power consumption and operating speed. Therefore, the depth model currently faces problems that are difficult to apply in the context of terminal deployment and low-latency requirements.

Therefore, model compression is an indispensable step for the depth model to be truly applied to the mobile terminal. In recent years, this field has achieved great development. The current mainstream compression methods include parameter pruning and sharing, low rank decomposition, and knowledge purification.In summary, the theoretical research on the depth model compression has been established at home and abroad, but compared to other tasks (such as computer vision, speech recognition, etc.), this field is still in an enlightening phase of development, especially in the depth learning model. The real mobile deployment is still very few. In the past two years, related research work has been gradually carried out. To successfully deploy mobile terminals, it is necessary to analyze the performance of different depth models on the one hand. On the other hand, the compression of models is also a necessary research point. Model compression is mainly about reducing the model size and reasoning time for experiments. By quantifying the shared weights, the size of the model can be compressed very well. By pruning the weights, the reasoning time of the model can be further accelerated. However, these methods mainly stay on the experimental conclusion and are not really on the mobile or embedded system. Deploying the model and putting it into practice, performance testing and compression experiments after actually deploying the model on an embedded platform is a necessary step to further verify the feasibility of the model's deployment on the mobile or embedded system, and is also the focus of this article.

In order to better deploy the model to a mobile or embedded system, This paper makes the following contributions:(1)\ Performance testing and analysis of the most widely used deep learning models in embedded systems, including inference time, model size, and time spent on each operand, and further analysis of these models in embedded systems Performance performance, related research work has not yet appeared, which is the first step in the migration of the deep learning model in the mobile platform.\ (2)\ Study the model compression technology based on deep learning, realize the model compression of several classical networks, compare the load characteristics of the model under different compression methods, mainly including the reasoning time change before and after the model compression, the model size change and the accuracy rate The loss, etc., resulting in different compression methods for different models of different applications of the effect of analysis of different models suitable for the compression method, making the use of different models in the mobile terminal is more likely.
