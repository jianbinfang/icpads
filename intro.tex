\section{Introduction}
%In recent years, CNN has become the main technical means of computer vision tasks, and it has been splendid in the direction of image classification, target detection, depth estimation, and semantic segmentation. In particular, since AlexNet won the ILSVRC 2012 ImageNet Image Classification Competition in one fell swoop, the deep neural network boom has swept the entire field of computer vision. Depth model swiftness replaces traditional manual design features and classifiers. Various depth models not only provide an end-to-end processing method, but also significantly refresh the accuracy of each task, and even beyond the accuracy of human knowledge. So far, the accuracy of the image classification model has reached more than 95\% (top5).

(Deep Learning has led an AI renaissance of sorts in recent years.
Image is one of its most prominent success area.
Results of the ImageNet Challenge points to a dramatic
improvement in object detection, localization and classification..
So far, the accuracy of the image classification model
has reached more than 95\% (top5)\FIXME{\cite{}}.
While, the excellent performance of these deep networks
depends on the powerful computing devices (i.e cloud platform)
to deal with a huge number of operations
for one inference (up to $O(10^9)$)\FIXME{\cite{}}.
It is expected that the size of the deep neural networks (i.e., number
of weights) will be larger for the more
difficult task than the simpler task and thus require
more energy to deal with the growing number of parameters and operations.


The current neural network based mobile
applications all rely on the cloud server
by putting the load on the server and waiting
for the inference results back to the mobile,
such as \FIXME{\cite{}} using the cloud-based APIs
to recognize the xxx lables from an image,
which leverages the power of cloud platform's machine learning
technology to give a high level of accuracy.
However, since the communication delay, the mobile applications
which provide neural network based services
can not satisfy the requirement for low latency.
Therefore, these deep neural networks
need to be migrated to mobile platforms to reduce the
inference time.
While the training and inference of
an advanced deep learning model usually requires
a large amount of computing resources, memory,
and computing power, which becomes a huge
obstacles for mobile devices and IoT devices.

Model compression techniques make it possible to run
the inferences on mobile devices by reducing the model size
and inference time.
In recent years, this field has achieved great development.
There are three mainstream compression methods,
parameter pruning and sharing\FIXME{\cite{}},
low rank decomposition\FIXME{\cite{}} and
knowledge purification\FIXME{\cite{}}.
However, these methods have not tested on the mobile or embedded system.
Our work focus on compressing the deep model and
testing the performance on mobile platform.
It is a necessary step to further verify the
feasibility of the model's deployment on the mobile
or embedded system.

In this paper, we compress 10 deep models which belong to four typical networks,
\texttt{mobilenet}, \texttt{vggnet}, \texttt{inception net}, \texttt{resnet} by two typical method, prune and quantization.
These models are meant to represent the state of
the art in deep learning. Most are
taken directly from top-tier research venues and have either
set new accuracy records on competitive datasets.
We focus on four areas: identifying the types of operations
which dominate execution time, compressing the different kinds of models,
retraining the compressed model to regain the accuracy,
comparing the model size and inference time before and after the compression.
We evaluate the compressed model on a representative heterogeneous mobile platform
NVIDIA Jetson TX2. In summary, our contributions are as follows:


\begin{itemize}
\item We test 10 most widely used deep
learning models in heterogeneous mobile platform.
Testing and analyzing inference time, model size, and
breakdown of execution time by operation type for each model.
We are the first to do such work,
which is the essential step in the migration of the
deep learning model in the mobile platform.
\item We compress the 10 models by quantization and pruning, and
comparing the inference time, model size and accuracy before and
after the compression for the first time..
\item We find that the inference
time varies for different models after using the quantification and prune.
and it can be further proved that the quantification
method has different effects on different models.
\end{itemize}


