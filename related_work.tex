\section{Related Work}
\DNNs have shown astounding successes in various tasks that previously seemed difficult~\cite{cho2014learning}. Despite the fact that many embedded
devices require precise sensing capabilities, adoption of \DNN models on such systems has notably slow progress. This mainly due to
\DNN-based inference being typically a computation intensive task, which inherently runs slowly on embedded devices due to limited
resources.

%
%There has been a significant amount of work on reducing the storage and computation requirement by model compression. For parameter
%sharing, Song et al.~\cite{han2015deep} proposed a method which quantizes the weights to enforce weight sharing and apply Huffman coding to
%compress model size. This method reduced the storage required by AlexNet by 35x and the size of VGG\_16 by 49x.
%In~\cite{Gong2014Compressing}, the authors tackled model storage issue by investigating information theoretical vector quantization methods
%for compressing the parameters of CNNs.} \FIXME{ With respect to pruning weights, ~\cite{Li2016Pruning} presented an acceleration method
%for CNNs, in which the authors pruned filters from CNNs that are identified as having a small effect on the output accuracy. By removing
%whole filters in the network together with their connecting feature maps, the computation costs are reduced significantly. Meantime, a
%LASSO regression based channel selection and least square reconstruction were presented by~\cite{he2017channel}, this method was applied to
%accelerate very deep convolutional neural networks and obtained promising results. } \FIXME{ Besides parameter sharing and pruning, the
%Low-rank factorization and Knowledge Distillation are also widely adopted in model compression. Denton et
%al.~\cite{denton2014exploiting}exploited the linear structure present within the convolutional filters to derive approximations that
%significantly reduce the required computation. ~\cite{lebedev2014speeding} proposed a simple two-step approach for speeding up convolution
%layers within large convolutional neural networks based on tensor decomposition and discriminative fine-tuning. A simple methodology to
%include a noise-based regularizer while training the student from the teacher was proposed in ~\cite{Sau2016Deep}, which provides a healthy
%improvement in the performance of the student network. Also, the same idea was proposed by ~\cite{Hinton2015Distilling}, which improved the
%acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single mode. }


There has been a significant amount of work on reducing the storage and computation work by model compression. These techniques include
pruning~\cite{Li2016Pruning}, quantization~\cite{Gong2014Compressing,han2015deep}, knowledge
distillation~\cite{Hinton2015Distilling,Sau2016Deep}, huffman coding\FIXME{~\cite{}}, low rank and sparse
decomposition~\cite{denton2014exploiting}, decomposition~\cite{lebedev2014speeding}, etc. This paper develops a quantitative approach to
understand the cost and benefits of deep learning compression techniques. We target pruning and data quantization because these are widely
used and directly applicable to a pre-trained model.



In addition to model compression, other works exploit computation-offloading~\cite{teerapittayanon2017distributed,Kang2017neurosurgeon},
specialised hardware design~\cite{chen2017eyeriss,Han:2016:EEI:3001136.3001163}, and dynamic model
selection~\cite{Taylor:2018:ADL:3211332.3211336}. Our work aims to understand how to accelerate deep learning inference by choosing the
right model compression technique. Thus, these approaches are orthogonal to our work.


As an alternative to on-device inferencing, off-loading computation to the cloud can accelerate \DNN model inference
\cite{teerapittayanon2017distributed}. Neurosurgeon \cite{Kang2017neurosurgeon} identifies when it is beneficial (\eg in terms of energy
consumption and end-to-end latency) to offload a \DNN layer to be computed on the cloud. The Pervasive \CNN~\cite{song2017towards}
generates multiple computation kernels for each layer of a \CNN, which are then dynamically selected according to the inputs and user
constraints. A similar approach presented in \cite{servia2017personal} trains a model twice, once on shared data and again on personal
data, in an attempt to prevent personal data being sent outside the personal domain. Computation off-loading is not always applicable due
to privacy, latency or connectivity issues. Our work is complementary to prior work on computation off-loading by offering insights to
choose the optimal compression technique to best optimize \emph{local} inference.



%As an alternative to on-device inferencing, off-loading computation to the cloud can accelerate \DNN model inference
%\cite{teerapittayanon2017distributed}. Neurosurgeon \cite{Kang2017neurosurgeon} identifies when it is beneficial (\eg in terms of energy
%consumption and end-to-end latency) to offload a \DNN layer to be computed on the cloud. The Pervasive \CNN~\cite{song2017towards} generates
%multiple computation kernels for each layer of a \CNN, which are then dynamically selected according to the inputs and user constraints. A
%similar approach presented in \cite{servia2017personal} trains a model twice, once on shared data and again on personal data, in an attempt to
%prevent personal data being sent outside the personal domain. Computation off-loading is not always applicable due to privacy, latency or
%connectivity issues. The work presented by Ossia \etal partially addresses the issue of privacy-preserving when offloading \DNN inference
%to the cloud ~\cite{osia2017hybrid}. Our work is complementary to prior work on computation off-loading by offering insights to choose the
%optimal compression technique to best optimize local inference.
