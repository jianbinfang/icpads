\begin{abstract}
The recent advances in deep neural networks (\DNNs) make them attractive for embedded systems. However, it can take a long time for DNNs
to make an inference on resource-constrained computing devices. Model compression techniques can address the computation issue of deep
inference on embedded devices. This technique is highly attractive, as it does not rely on specialized hardware, or
computation-offloading that is often infeasible due to privacy concerns or high latency. However, it remains unclear how model
compression techniques perform across a wide range of \DNNs. To design efficient embedded deep learning solutions, we need to understand
their behaviors. This work develops a quantitative approach to characterize model compression techniques on a representative embedded
deep learning architecture, the NVIDIA Jetson Tx2. We perform extensive experiments by considering 11 influential neural network
architectures from the image classification and the natural language processing domains. We experimentally show that how two mainstream
compression techniques, data quantization and pruning, perform on these network architectures and the implications of compression
techniques to the model storage size, inference time, energy consumption and performance metrics. We demonstrate that there are
opportunities to achieve fast deep inference on embedded systems, but one must carefully choose the compression settings. Our results
provide insights on when and how to apply model compression techniques and guidelines for designing efficient embedded deep learning
systems.
\end{abstract}
