\begin{abstract}
The recent advances in deep neural networks (\DNNs) make them attractive for embedded systems. However, it can take a long time for DNNs
to make an inference on resource-constrained mobile and IoT devices. Offloading the computation into the cloud is often infeasible due to
privacy concerns, high latency, or the lack of connectivity. As such, there is a critical need to find a way to effectively execute the
DNN models locally on the devices.

Model compression techniques can address the computation issue of deep inference on embedded devices without requiring specialized
hardware. However, it remains unclear how model compression techniques perform across a wide range of \DNNs. To design efficient embedded
deep learning solutions, we need to understand their behaviors. This work develops a quantitative approach to characterize model
compression techniques on a representative embedded deep learning architecture, the NVIDIA Jetson Tx2. We perform extensive experiments
by considering 13 influential neural network architectures from the image classification and the natural language processing
domains. We experimentally show that how two mainstream compression techniques, data quantization and pruning, perform on these network
architectures and the implications compression techniques to the \DNN storage size, inference time, energy consumption and performance
metrics. We demonstrate that there is a potential to achieve fast deep inference on embedded systems, but one must carefully choose the
parameter settings of model compression. Our results provide insights on when and how to apply model compression techniques and
guidelines for designing efficient embedded deep learning systems.
\end{abstract}
