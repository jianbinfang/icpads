\begin{figure*}[!t]
\centering
\subfloat[][Model size]{\includegraphics[width=0.33\textwidth]{figure/quan_size.pdf}}
\hfill
\subfloat[][Inference time]{\includegraphics[width=0.34\textwidth]{figure/quan_time2.pdf}}
\hfill
\subfloat[][Accuracy]{\includegraphics[width=0.32\textwidth]{figure/quan_acc3.pdf}}
\hfill
\subfloat[][Power consumption]{\includegraphics[width=0.33\textwidth]{figure/quan_power2.pdf}}
\hfill
\subfloat[][Energy consumption]{\includegraphics[width=0.34\textwidth]{figure/quan_energy2.pdf}}
\hfill
\subfloat[][precision, recall and F1 score]{\includegraphics[width=0.33\textwidth]{figure/quan_prf2.pdf}}
\hfill

\caption{The achieved model size (a) inference time (b) accuracy (c) power consumption (d)
energy consumption (e) and precision, recall and F1 score (e) before and after the compression by \quantization.
The compression technique to use depends on the optimization target.}
\label{fig:analy_quan}
\end{figure*}


\begin{figure*}[!t]
\centering
\subfloat[][Model size]{\includegraphics[width=0.33\textwidth]{figure/prun_size.pdf}}
\hfill
\subfloat[][Inference time]{\includegraphics[width=0.3\textwidth]{figure/prun_time.pdf}}
\hfill
\subfloat[][Accuracy]{\includegraphics[width=0.3\textwidth]{figure/top1_5_prun.pdf}}
\hfill
\subfloat[][Power consumption]{\includegraphics[width=0.3\textwidth]{figure/prun_power.pdf}}
\hfill
\subfloat[][Energy consumption]{\includegraphics[width=0.3\textwidth]{figure/prun_energy.pdf}}
\hfill
\subfloat[][precision, recall and F1 score]{\includegraphics[width=0.3\textwidth]{figure/prun_prf.pdf}}
\hfill

\caption{The change of the model size (a), inference time (b), accuracy/BLEU (c), power (d), energy consumption (e), and accuracy (e)
before and after applying \pruning.} \label{fig:analy_prun}
\end{figure*}

\section{Experimental Results}


\subsection{Roadmap}
Our experiments try to answer the following questions:

\begin{itemize}
\item bla
\item bla2
\item bla3
\end{itemize}

\subsection{Impact on the Model Storage Size}
Reducing the model storage size is crucial for embedded and IoT systems which often have a limited storage space. A smaller model size also
translates to smaller runtime memory footprint of less RAM space consumption. Figures~\ref{fig:analy_quan} and  \ref{fig:analy_prun}
illustrate how the different compression techniques and parameters affect the resulting model size.

As can be seen from Figure~\ref{fig:analy_quan} a, using an 8-bit data quantization can significantly reduce the model storage size,
leading to an average reduction of 74.5\%. From Figure~\ref{fig:analy_prun} a, we see that by removing some of the pathways of the neural
network, \pruning can also reduce the model size, although the gain is smaller than \quantization. On average, \pruning reduces the model
size by 27.2\% (\FIXME{xx MB}). An interesting observation is that, \pruning is particularly effective for obtaining a compact model for
NMT, an \RNN, with a reduction of 60\% on the model size. This is because there are typically many repetitive pathways in an \RNN due to
the natural of the network architecture. As we will discuss later, \pruning only leads to a minor degradation in the prediction accuracy
for NMT. This suggests that \pruning can be an effective model compression technique for \RNNs.



\subsection{Impact on Inference Time}
Theoretically, the compressed models take less time to make an inference, as 
the redundent contents have been represented by using less bits or removed.
Figure~\ref{fig:analy_quan} b compares the inference time
among three compression radios: 16-bit, 8-bit and 6-bit, while all of them 
increased the inference time because of two major overhead.
Firstly, the model will perfrom data quantization by using less bit (eg. from 32-bit to 8-bit) on
the input test float data. This process takes about 15\% of total inference time.
Secondly, in order to guarantee the accuracy, we find that a conversion function will be added into 
the graph of quantized model. This conversion function are used to convert the 8-bit to 32-bit in the output layer which 
occupies from 30\% to 50\%.
The two major overhead counteract the speedup by using the less-bit to represent the original 32-bit float data. 
Even so, using an 6-bit data quantization is the fastest for inferencing,
being xx and xx faster than 16-bit and 8-bit, respectively, 
but is the least accurate (see Figure~\ref{fig:analy_quan} c). 

Figure~\ref{fig:analy_prun} b presents the reduced inference time by \pruning.
\pruning reduced the inference time by pruning the redundent paramenters in the models,
we can see from the bar chart that
Vgg\_16 and NMT are reduced most time, both of them with an reduction of 38\%, and 
Resnet\_50 reduced 20\% inference time. Overall, the average inference time is reduced by 31\%.


\subsection{Impact on Accuracy Metrics}
When compressing a model, we still want to largely maintain the performance of a compressed model. Therefore, it is importance to
effectively trade precision for storage space. Results in Figure~\ref{fig:analy_quan}c compare how the prediction accuracy metrics are affected by model
compression.

We see that the sweat spot of \quantization depends on the neural network structure. Although an 8-bit representation leads to a minor
decrease in the prediction accuracy, a further reduction of a 6-bit representation is only profitable for xx networks.



\subsection{Impact on the Energy Consumption}


\subsection{Memory footprint}

\begin{figure}[!t]
\centering
\subfloat[][\quantization]{\includegraphics[width=0.4\textwidth]{figure/quan_mem.pdf}}
\hfill
\subfloat[][\pruning]{\includegraphics[width=0.4\textwidth]{figure/prun_mem.pdf}}
\hfill

\caption{Memory footprint before and after the compression by \quantization(a) and \pruning (b).}
\label{fig:footprint}
\end{figure}
