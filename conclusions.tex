\section{Conclusions}
This paper has presented a comprehensive study to characterize the effectiveness of model compression techniques on embedded systems. We
consider two mainstream model compression techniques and apply them to a wide range of representative deep neural network architectures. We
show that there is no ``on-fits-for-all" universal compression setting, and the right decision depends on the target neural network
architecture and the optimization constraints. We reveal the cause of the performance disparity and demonstrate that a carefully chosen
parameter setting can lead to efficient embedded deep inference. We provide new insights and concrete guidelines, and define possible
avenues of research to enable efficient embedded inference.
